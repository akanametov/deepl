{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deepl.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF_B3pwr8ZDl",
        "outputId": "56ac49f0-b72e-419e-d037-53d215969dde"
      },
      "source": [
        "!pip install git+https://github.com/akanametov/deepl.git#egg=deepl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepl\n",
            "  Cloning https://github.com/akanametov/deepl.git to /tmp/pip-install-kvu8q2w6/deepl\n",
            "  Running command git clone -q https://github.com/akanametov/deepl.git /tmp/pip-install-kvu8q2w6/deepl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepl) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from deepl) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->deepl) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->deepl) (1.0.1)\n",
            "Building wheels for collected packages: deepl\n",
            "  Building wheel for deepl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepl: filename=deepl-0.1-cp37-none-any.whl size=11430 sha256=8f5c890c6e60695ba25e7fef32591e823ce364a8475a5860a417808a57c46722\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ootuamdi/wheels/08/fe/70/dfa6150a653ff950af33a7358c093de4e8af97b42192bd6420\n",
            "Successfully built deepl\n",
            "Installing collected packages: deepl\n",
            "Successfully installed deepl-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRnEoUhV8i3A",
        "outputId": "b7e102b6-0f00-4a38-f213-7e788024105c"
      },
      "source": [
        "!wget https://pjreddie.com/media/files/mnist_train.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-10 09:04:17--  https://pjreddie.com/media/files/mnist_train.csv\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 109575994 (104M) [application/octet-stream]\n",
            "Saving to: ‘mnist_train.csv’\n",
            "\n",
            "mnist_train.csv     100%[===================>] 104.50M  31.1MB/s    in 3.7s    \n",
            "\n",
            "2021-06-10 09:04:21 (28.0 MB/s) - ‘mnist_train.csv’ saved [109575994/109575994]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpbOqZIh8szY"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "train = pd.read_csv('sample_data/mnist_train_small.csv', header=None).sample(1000)\n",
        "assert len(np.unique(train.iloc[:, 0])) == 10\n",
        "\n",
        "x = train.drop(labels=0, axis=1).to_numpy()\n",
        "y = train[0].values.copy()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU1btozI9Ay8"
      },
      "source": [
        "from deepl import nn\n",
        "from deepl.nn import functional as F\n",
        "from deepl.criterion import CrossEntropyLoss\n",
        "from deepl.metrics import accuracy\n",
        "from deepl.optimizer import Adam\n",
        "from deepl.data import Dataset, DataLoader\n",
        "\n",
        "# define network architecture\n",
        "class CNN(nn.Network):\n",
        "    def __init__(self,):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 4, kernel_size=(3,3), stride=(2,2))\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(4, 6, kernel_size=(3,3), stride=(2,2))\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.conv3 = nn.Conv2d(6, 8, kernel_size=(3,3), stride=(2,2))\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear1 = nn.Linear(8*2*2, 16)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc = nn.Linear(16, 10)\n",
        "        \n",
        "    def forward(self, x): # define forward path\n",
        "        \n",
        "        fx = self.conv1(x)\n",
        "        fx = self.relu1(fx)\n",
        "        fx = self.conv2(fx)\n",
        "        fx = self.relu2(fx)\n",
        "        fx = self.conv3(fx)\n",
        "        fx = self.relu3(fx)\n",
        "        \n",
        "        fx = self.flatten(fx)\n",
        "        \n",
        "        fx = self.linear1(fx)\n",
        "        fx = self.relu4(fx)\n",
        "        fx = self.fc(fx)\n",
        "        \n",
        "        return fx\n",
        "\n",
        "# define your dataset\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, x, y, mode='train'):\n",
        "        super().__init__()\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.mode=mode\n",
        "        \n",
        "    def __len__(self,):\n",
        "        return len(self.x)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        #print(idx)\n",
        "        xi = self.x[idx]#.iloc[idx].values\n",
        "        #print(xi.shape)\n",
        "        xi = xi.reshape(len(xi), 1, 28, 28)/255\n",
        "        if self.mode=='test':\n",
        "            return xi\n",
        "        yi = self.y[idx].astype(int)\n",
        "        return xi, yi"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9ATDkQuA54k"
      },
      "source": [
        "dataset = MyDataset(x, y)\n",
        "dataloader = DataLoader(dataset, batch_size=50)\n",
        "\n",
        "# assign model, critreion and optimizer\n",
        "model = CNN()\n",
        "criterion = CrossEntropyLoss()\n",
        "optimizer = Adam(lr=0.0005)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwt8n9rZA59G",
        "outputId": "e125f21b-e2f5-43e6-c74c-e3632d39c86a"
      },
      "source": [
        "epochs=100\n",
        "history={'loss':[], 'acc':[]}\n",
        "\n",
        "# Train loop\n",
        "for epoch in range(epochs):\n",
        "    e_loss, e_acc = 0., 0.\n",
        "    for (x, y) in dataloader:\n",
        "        p = model(x)#.flatten()                   # obtain prediction\n",
        "        loss = criterion(p, y)                   # calculate loss \n",
        "        acc = accuracy(np.argmax(p,axis=1), y) # calculate accuracy\n",
        "        grad = criterion.backward()              # get gradient of loss function \n",
        "        model.backward(grad)                     # backward propogate of gradient through network\n",
        "        params = optimizer.step(model.parameters(), model.grads()) # calculate new parameters by optimizer\n",
        "        model.update(params)                     # update parameters of network\n",
        "        \n",
        "        e_loss += loss.item()/len(dataloader)\n",
        "        e_acc += acc.item()/len(dataloader)\n",
        "    print(f'Epoch {epoch+1}/{epochs} | loss: {e_loss:.4f} | acc: {e_acc:.4f}')\n",
        "    history['loss'].append(e_loss)               # save loss value\n",
        "    history['acc'].append(e_acc)                 # save accuracy value"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | loss: 2.3210 | acc: 0.1210\n",
            "Epoch 2/100 | loss: 2.2941 | acc: 0.1850\n",
            "Epoch 3/100 | loss: 2.2794 | acc: 0.1820\n",
            "Epoch 4/100 | loss: 2.2638 | acc: 0.1890\n",
            "Epoch 5/100 | loss: 2.2400 | acc: 0.2120\n",
            "Epoch 6/100 | loss: 2.2028 | acc: 0.2340\n",
            "Epoch 7/100 | loss: 2.1405 | acc: 0.3040\n",
            "Epoch 8/100 | loss: 2.0429 | acc: 0.3500\n",
            "Epoch 9/100 | loss: 1.9130 | acc: 0.3650\n",
            "Epoch 10/100 | loss: 1.7878 | acc: 0.3900\n",
            "Epoch 11/100 | loss: 1.6743 | acc: 0.4460\n",
            "Epoch 12/100 | loss: 1.5689 | acc: 0.4950\n",
            "Epoch 13/100 | loss: 1.4818 | acc: 0.5280\n",
            "Epoch 14/100 | loss: 1.4093 | acc: 0.5510\n",
            "Epoch 15/100 | loss: 1.3508 | acc: 0.5840\n",
            "Epoch 16/100 | loss: 1.3023 | acc: 0.6000\n",
            "Epoch 17/100 | loss: 1.2610 | acc: 0.6160\n",
            "Epoch 18/100 | loss: 1.2248 | acc: 0.6230\n",
            "Epoch 19/100 | loss: 1.1927 | acc: 0.6360\n",
            "Epoch 20/100 | loss: 1.1638 | acc: 0.6370\n",
            "Epoch 21/100 | loss: 1.1374 | acc: 0.6430\n",
            "Epoch 22/100 | loss: 1.1125 | acc: 0.6530\n",
            "Epoch 23/100 | loss: 1.0895 | acc: 0.6590\n",
            "Epoch 24/100 | loss: 1.0681 | acc: 0.6650\n",
            "Epoch 25/100 | loss: 1.0484 | acc: 0.6700\n",
            "Epoch 26/100 | loss: 1.0299 | acc: 0.6790\n",
            "Epoch 27/100 | loss: 1.0125 | acc: 0.6830\n",
            "Epoch 28/100 | loss: 0.9959 | acc: 0.6890\n",
            "Epoch 29/100 | loss: 0.9802 | acc: 0.6970\n",
            "Epoch 30/100 | loss: 0.9653 | acc: 0.7050\n",
            "Epoch 31/100 | loss: 0.9511 | acc: 0.7120\n",
            "Epoch 32/100 | loss: 0.9378 | acc: 0.7100\n",
            "Epoch 33/100 | loss: 0.9253 | acc: 0.7140\n",
            "Epoch 34/100 | loss: 0.9134 | acc: 0.7190\n",
            "Epoch 35/100 | loss: 0.9021 | acc: 0.7210\n",
            "Epoch 36/100 | loss: 0.8914 | acc: 0.7230\n",
            "Epoch 37/100 | loss: 0.8814 | acc: 0.7270\n",
            "Epoch 38/100 | loss: 0.8719 | acc: 0.7290\n",
            "Epoch 39/100 | loss: 0.8628 | acc: 0.7320\n",
            "Epoch 40/100 | loss: 0.8539 | acc: 0.7360\n",
            "Epoch 41/100 | loss: 0.8453 | acc: 0.7390\n",
            "Epoch 42/100 | loss: 0.8369 | acc: 0.7390\n",
            "Epoch 43/100 | loss: 0.8289 | acc: 0.7400\n",
            "Epoch 44/100 | loss: 0.8208 | acc: 0.7460\n",
            "Epoch 45/100 | loss: 0.8132 | acc: 0.7440\n",
            "Epoch 46/100 | loss: 0.8057 | acc: 0.7480\n",
            "Epoch 47/100 | loss: 0.7987 | acc: 0.7540\n",
            "Epoch 48/100 | loss: 0.7917 | acc: 0.7560\n",
            "Epoch 49/100 | loss: 0.7851 | acc: 0.7540\n",
            "Epoch 50/100 | loss: 0.7783 | acc: 0.7540\n",
            "Epoch 51/100 | loss: 0.7717 | acc: 0.7550\n",
            "Epoch 52/100 | loss: 0.7651 | acc: 0.7560\n",
            "Epoch 53/100 | loss: 0.7592 | acc: 0.7580\n",
            "Epoch 54/100 | loss: 0.7539 | acc: 0.7600\n",
            "Epoch 55/100 | loss: 0.7492 | acc: 0.7650\n",
            "Epoch 56/100 | loss: 0.7447 | acc: 0.7680\n",
            "Epoch 57/100 | loss: 0.7403 | acc: 0.7730\n",
            "Epoch 58/100 | loss: 0.7360 | acc: 0.7720\n",
            "Epoch 59/100 | loss: 0.7319 | acc: 0.7740\n",
            "Epoch 60/100 | loss: 0.7282 | acc: 0.7770\n",
            "Epoch 61/100 | loss: 0.7246 | acc: 0.7800\n",
            "Epoch 62/100 | loss: 0.7212 | acc: 0.7790\n",
            "Epoch 63/100 | loss: 0.7176 | acc: 0.7800\n",
            "Epoch 64/100 | loss: 0.7139 | acc: 0.7810\n",
            "Epoch 65/100 | loss: 0.7116 | acc: 0.7840\n",
            "Epoch 66/100 | loss: 0.7066 | acc: 0.7860\n",
            "Epoch 67/100 | loss: 0.7027 | acc: 0.7860\n",
            "Epoch 68/100 | loss: 0.6992 | acc: 0.7880\n",
            "Epoch 69/100 | loss: 0.6956 | acc: 0.7870\n",
            "Epoch 70/100 | loss: 0.6921 | acc: 0.7880\n",
            "Epoch 71/100 | loss: 0.6886 | acc: 0.7890\n",
            "Epoch 72/100 | loss: 0.6851 | acc: 0.7910\n",
            "Epoch 73/100 | loss: 0.6817 | acc: 0.7910\n",
            "Epoch 74/100 | loss: 0.6784 | acc: 0.7940\n",
            "Epoch 75/100 | loss: 0.6752 | acc: 0.7950\n",
            "Epoch 76/100 | loss: 0.6721 | acc: 0.7940\n",
            "Epoch 77/100 | loss: 0.6691 | acc: 0.7960\n",
            "Epoch 78/100 | loss: 0.6663 | acc: 0.7980\n",
            "Epoch 79/100 | loss: 0.6637 | acc: 0.8010\n",
            "Epoch 80/100 | loss: 0.6612 | acc: 0.8030\n",
            "Epoch 81/100 | loss: 0.6587 | acc: 0.8050\n",
            "Epoch 82/100 | loss: 0.6562 | acc: 0.8070\n",
            "Epoch 83/100 | loss: 0.6535 | acc: 0.8070\n",
            "Epoch 84/100 | loss: 0.6509 | acc: 0.8060\n",
            "Epoch 85/100 | loss: 0.6484 | acc: 0.8060\n",
            "Epoch 86/100 | loss: 0.6460 | acc: 0.8080\n",
            "Epoch 87/100 | loss: 0.6437 | acc: 0.8090\n",
            "Epoch 88/100 | loss: 0.6414 | acc: 0.8080\n",
            "Epoch 89/100 | loss: 0.6392 | acc: 0.8090\n",
            "Epoch 90/100 | loss: 0.6370 | acc: 0.8100\n",
            "Epoch 91/100 | loss: 0.6349 | acc: 0.8090\n",
            "Epoch 92/100 | loss: 0.6329 | acc: 0.8080\n",
            "Epoch 93/100 | loss: 0.6310 | acc: 0.8080\n",
            "Epoch 94/100 | loss: 0.6291 | acc: 0.8090\n",
            "Epoch 95/100 | loss: 0.6273 | acc: 0.8070\n",
            "Epoch 96/100 | loss: 0.6255 | acc: 0.8070\n",
            "Epoch 97/100 | loss: 0.6239 | acc: 0.8070\n",
            "Epoch 98/100 | loss: 0.6222 | acc: 0.8070\n",
            "Epoch 99/100 | loss: 0.6206 | acc: 0.8080\n",
            "Epoch 100/100 | loss: 0.6191 | acc: 0.8070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsumOhQhAUK1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}